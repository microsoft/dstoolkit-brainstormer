{
 "0-0": {
  "ip_title": "Anomaly Detection",
  "toolkit_url" : "https://www.ds-toolkit.com/assets/165c1cfd-91e6-4f2f-b662-c072b194bf1e",
  "github_url": "https://github.com/microsoft/dstoolkit-anomaly-detection-ijungle",
  "scenarios": "Anomaly detection is the ability to identify rare items or observations that don't conform to normal or common patterns found in data. These outliers are important because they can indicate potential risks, control failures, or business opportunities. Moreover, they usually represent critical areas that need further examination or scrutiny. These events, characterized for being statistically different from the rest of the observations typically translate to a business problem such as: credit card fraud, failing machine in a server, a cyber-attack, variation in financial transactions However, sometimes they can also point out to new ways for doing things in a more effective way, like spotting productivity spikes. Common Anomaly Detection techniques are difficult to implement on very large sets of Data. The Anomaly Detection Accelerator, leverages the iJungle technique from Dr. Ricardo Castro, which solves this challenge, enabling anomaly detection on large sets of data.",
  "business_value": "Anomalies can negatively impact the business and generate huge costs. Thus, the accelerator can improve cost control and reduce surges in cost. Reduced cost, Reduced risk",
  "asset_description": "Anomaly detection is usually executed as a single threaded operation. The iJungle Anomaly Detection Approach uses a technique that enables parallel processing of anomaly detection, greatly increasing the speed at which anomalies can be detected on very large datasets. The code can be run in an on-premise setting or in the cloud. The repository includes a tutorial notebook for leveraging Azure Machine Learning capabilities like parallel training, and parallel evaluation to be able to reach high volume data analysis. It includes examples of how to use the iJungle approach as a series of notebooks for Azure Machine Learning and Azure Databricks",
  "modeling_approach_and_training": "Training and Inference Pipeline python / synapse notebooks are provided; these import the iJungle library to show how it is used on sample datasets.",
  "dataset_summary": "Tabular data containing outliers. The data is provided in the repository",
  "architecture": "Azure Machine Learning, Azure Databricks",
  "topic": "Anomaly Detection",
  "explanation": "Anomaly detection is the process of identifying rare items or observations that do not conform to normal patterns in data. These outliers can indicate potential risks, control failures, or business opportunities. The Anomaly Detection Accelerator leverages the iJungle technique to enable anomaly detection on large datasets, which can help reduce costs and risks.",
  "short_explanation": "Anomaly detection identifies rare items or observations that do not conform to normal patterns in data. The Anomaly Detection Accelerator leverages the iJungle technique to enable anomaly detection on large datasets, which can help reduce costs and risks.",
  "top_keywords": "anomaly detection, outliers, risks, cost control, iJungle",
  "ip_id": 0
 },
 "1-1": {
  "ip_title": "Azure Data Labs",
  "github_url": "https://github.com/Azure/azure-data-labs-modules",
  "toolkit_url" : "https://www.ds-toolkit.com/assets/75168078-ff28-4ea8-8f13-6ca632c80f2d",
  "scenarios": "Consistency in deployment, cost control, compliance and security are challenges for IT teams especially when business requires data labs or environments in which to create their data products.",
  "business_value": "Consistency in deployment, cost control, compliance and security are challenges for IT teams especially when business requires data labs or environments in which to create their data products.",
  "asset_description": "Azure Data Labs includes a library of curated Terraform modules as well as a Continuous Integration (CI) environment using GitHub Actions for modules' validation and deployment.",
  "modeling_approach_and_training": "na",
  "dataset_summary": "na",
  "architecture": "Azure Data Services, GitHub Actions, Terraform",
  "topic": "Continuous Integration (CI) using GitHub Actions for Terraform modules",
  "explanation": "Continuous Integration (CI) using GitHub Actions for Terraform modules helps ensure consistency in deployment, cost control, compliance, and security for IT teams creating data labs or environments for data product development.",
  "short_explanation": "CI for Terraform modules ensures consistent, secure data lab deployment.",
  "top_keywords": "Continuous Integration, GitHub Actions, Terraform modules, deployment consistency, secure data lab",
  "ip_id": 1
 },
 "2-2": {
  "ip_title": "Azure Object Detection",
  "toolkit_url" : "https://www.ds-toolkit.com/assets/e592804c-7db8-4266-a579-6155be176f2b",
  "github_url": "https://github.com/microsoft/dstoolkit-azoda",
  "scenarios": "Solve business use-cases that include: Locating a target object or objects in images; Differentiate object types in images; Determine the sizes of objects in images",
  "business_value": "Automate visual tasks for example,Product inspection Quality control / Defect detection; Item counting; Document analysis;Medical image analysis; Store surveilance",
  "asset_description": "This repository serves as a starting point and accelerator for object detection projects on Azure. The walkthrough will help quickly setup training, labelling, inference, testing and deployment pipelines within a few minutes of setup. A starter dataset is generated for demonstration purposes to illustrate where to place project data. The pipelines clearly show the steps required so that changes can be easily made as required.",
  "modeling_approach_and_training": "The repo maintains a collection of different models. Pipelines automate the training. Users can configure the pipelines to use different models and datasets.",
  "dataset_summary": "A synthetic dataset is provided as a starting point and instructions are given on how to replace the dataset with project specific data",
  "architecture": "Azure DevOps, Azure Machine Learning, Custom Vision, Yolo V5, Yolo V7, Yolo V8",
  "topic": "Object Detection",
  "explanation": "Object detection is a computer vision technique that involves identifying objects in images or videos and locating them. It can be used to automate visual tasks such as product inspection, quality control, item counting, document analysis, medical image analysis, and store surveillance.",
  "short_explanation": "Automate visual tasks using object detection.",
  "top_keywords": "Computer Vision, Image Analysis, Object Identification, Automation, Surveillance",
  "ip_id": 2
 },
 "3-3": {
  "ip_title": "Azure OpenAI App Demo",
  "github_url": "https://github.com/microsoft/dstoolkit-AOAIFlaskApp",
  "toolkit_url" : "https://www.ds-toolkit.com/assets/c5751b2c-c2dc-4234-929b-ee2f8eb272e3",
  "scenarios": "Showcasing various functionalities and use cases of Azure OpenAI",
  "business_value": "Demonstration of the new Azure OpenAI service and its offerings.",
  "asset_description": "1. Call Centre Analytics\n2. NLP to SQL 3. NLP to Python 4. Synthetic Data Generation\n5. User Stories 6. Text Generation 7. Classify Text\n8. Similarity Embedding 9. Text Search\n10. Entity extraction",
  "modeling_approach_and_training": "Azure OpenAI API calls",
  "dataset_summary": "Most data is via OpenAI which is publicly available data scraped from the Internet. Once tabular dataset is also utilized, which is a publicly available dataset downloaded from Kaggle.",
  "architecture": "Azure OpenAI",
  "topic": "Natural Language Processing (NLP)",
  "explanation": "NLP is a branch of AI that helps machines understand, interpret, and manipulate human language. This demo showcases various NLP functionalities and use cases of Azure OpenAI, such as call center analytics, text generation, and entity extraction.",
  "short_explanation": "Azure OpenAI demo showcasing NLP functionalities for call center analytics, text generation, and entity extraction.",
  "top_keywords": "Azure, OpenAI, NLP, call center analytics, text generation, entity extraction",
  "ip_id": 3
 },
 "4-4": {
  "ip_title": "Consumer Analytics",
  "github_url": "https://github.com/microsoft/dstoolkit-ai-ux/tree/main/visualizations/consumer_analytics",
  "toolkit_url" : "https://www.ds-toolkit.com/assets/ef47980f-2a62-4854-b3dd-da15429bff7f",
  "scenarios": "It can be difficult to determine how much you spend on marketing when targeting customers. In this accelerator you can learn more about your customers with regards to how likely they are to churn and what their customer lifetime value will be. As the churn probability of a customer increases, consider marketing intervention to prevent customer churn.",
  "business_value": "- New insights into customer spending habits;More effective apportioning of marketing budgets",
  "asset_description": "This is a self service tool to allow you to upload your own customer data or use sample data to see customer insights with an interactive tool to allow you to determine customer lifetime value given a customer's spending habits",
  "modeling_approach_and_training": "This is a simple flask application that wraps python's lifetimes library. It uses a Beta-Geometric to determine customer churn and combines this with a Gamma-Gamma model of monetary value to determine Customer Lifetime Value. Models are trained as data is uploaded and currently there is no caching of data.",
  "dataset_summary": "Tabular data. The data required for this accelerator is the frequency (number of transactions), the recency in weeks (how recently did they last purchase), the time since their first purchase in weeks, and the average monetary value of their transactions.",
  "architecture": "Azure App Services, flask",
  "topic": "Customer Churn Prediction and Lifetime Value Analysis",
  "explanation": "This accelerator helps businesses to predict the likelihood of customer churn and determine their lifetime value based on their spending habits. By identifying customers with a high probability of churn, businesses can intervene with targeted marketing strategies to prevent churn and allocate marketing budgets more effectively.",
  "short_explanation": "Accelerator for predicting customer churn and lifetime value based on spending habits.",
  "top_keywords": "customer churn, lifetime value, spending habits, marketing intervention, customer insights",
  "ip_id": 4
 },
 "5-5": {
  "ip_title": "Conversational AI Advanced processing service",
  "github_url": "https://github.com/microsoft/cai-advanced-processing-service",
  "toolkit_url" : "https://www.ds-toolkit.com/assets/6823d216-7e9c-40c9-9b9e-27dff2e5dd8b",
  "scenarios": "Conversational AI today is built on top of many connected technologies from several providers. This makes access validation, identification and authentication more difficult. This accelerator will simplify that workflow, adding security and speeding time to authentication.",
  "business_value": "The main field of use are intelligent applications with text-and speech input, such as chat bots or voice bots.Provide a modular and extendable pre/post processing serviceProvide a better experience of the app with more flexible user input options;Enable extended, context-based understanding of user input;Take a channel-based approach where appropriate;Process/UX Flow Best Practices for common scenarios;Create a consistent experience;Reduce errors",
  "asset_description": "This solution accelerator is designed for use with intelligent applications with text and speech input, such as chat bots or voice bots. It consists of a series of APIs the simplify common validation, identification and authentication tasks when implementing conversational AI solutions.",
  "modeling_approach_and_training": "There are 3 layers, API Layer - Contains function-specific Logic (e.g. Business rules like IBAN structure) and the html request and response handling when using an http-triggered function.;Module Layer - Contains re-usable Processing Logic (e.g. Spelling Resolver). A module can be based on Python classes or functions.;Import Layer - Stores data assets required for the processing (e.g. spelling dictionaries, spelling rules, address table). These can also be split by languages, if support for multiple languages is required.",
  "dataset_summary": "Textual data",
  "architecture": "Azure Functions, Azure KeyVault, Azure Language Understanding Service (LUIS), Azure Table Storage, Python",
  "topic": "Conversational AI",
  "explanation": "Conversational AI is a technology that enables intelligent applications with text and speech input, such as chat bots or voice bots. This accelerator simplifies access validation, identification, and authentication, adding security and speeding up the authentication process.",
  "short_explanation": "Conversational AI accelerator simplifies authentication for chat bots and voice bots.",
  "top_keywords": "Conversational AI, chat bots, voice bots, authentication, security",
  "ip_id": 5
 },
 "6-6": {
  "ip_title": "Corporate Financial Forecasting on Azure Synapse",
  "github_url": "https://github.com/microsoft/dstoolkit-corporate-financial-forecasting",
  "toolkit_url" : "https://www.ds-toolkit.com/assets/eb16ae45-74f4-46e8-a2fc-369b5e23233a",
  "scenarios": "Corporate Financial Forecasting on Azure Synapse",
  "business_value": "More accurate forecasts in 95% less time.",
  "asset_description": "This accelerator contains examples of how to implement Microsoft Finance's time series forecasting framework, aka Finn, through our open-source R package called finnts.",
  "modeling_approach_and_training": "Step by step approach to leveraging the finnts package in Azure Synapse Notebooks.",
  "dataset_summary": "na",
  "architecture": "Azure Synapse, Azure Data Lake Storage, R",
  "topic": "Time Series Forecasting",
  "explanation": "Time series forecasting is a data science technique used to predict future values based on historical data patterns. In this case, Microsoft Finance's time series forecasting framework, Finn, is implemented through an open-source R package called finnts to provide more accurate forecasts in 95% less time for corporate financial forecasting on Azure Synapse.",
  "short_explanation": "Time series forecasting for accurate corporate financial forecasting on Azure Synapse.",
  "top_keywords": "Time Series Forecasting, Microsoft Finance, Finn, Open-source R package, Corporate Financial Forecasting",
  "ip_id": 6
 },
 "7-7": {
  "ip_title": "Data Discovery Toolkit",
  "github_url": "https://github.com/microsoft/Data-Discovery-Toolkit",
  "toolkit_url" : "https://www.ds-toolkit.com/assets/49a0f600-0394-4525-b1b0-98d90d7fdc63",
  "scenarios": "Every machine learning project requires a deep understanding of the data, to be able to understand whether the data is representative of the problem to be solved, to determine the approaches to be undertaken and indeed for the project to be successful.\nIt is complex part of the project where data is attempted to be cleansed, outliers identified and the suitability of the data is assessed to inform",
  "business_value": "The Data Discovery Playbook aims to quickly provide structured views on your text, images and videos, all at scale using Synapse and unsupervised ML techniques that exploit state of the art deep learning models.",
  "asset_description": "The aim of this Playbook is to illustrate the usage of the tools, alongside guidance, examples and documentation to get rapid insights of your unstructed data, all of which have been applied in real customer solutions.",
  "modeling_approach_and_training": "Azure Synapse notebooks and Spark Pools for data processing and compute.\nPowerBI for rapid and simple interactive data visualisation dashboards\nOpenAI These models can be easily adapted to your specific task including but not limited to content generation, summarization, semantic search, and natural language to code translation\nKeras Applications in particular InceptionV3 for image inference",
  "dataset_summary": "na",
  "architecture": "Azure Synapse notebooks and Spark Pools for data processing and compute.\nPowerBI for rapid and simple interactive data visualisation dashboards\nOpenAI These models can be easily adapted to your specific task including but not limited to content generation.",
  "topic": "Data Discovery using Unsupervised Machine Learning",
  "explanation": "Data Discovery is the process of understanding the data to determine its suitability for a machine learning project. Unsupervised Machine Learning techniques are used to quickly provide structured views on unstructured data such as text, images, and videos, all at scale using Synapse.",
  "short_explanation": "Data Discovery using Unsupervised ML for unstructured data at scale using Synapse.",
  "top_keywords": "Data Discovery, Unsupervised Machine Learning, Synapse, Structured Views, Unstructured Data",
  "ip_id": 7
 },
 "8-8": {
  "ip_title": "Energy Profiling & Prediction",
  "github_url": "https://github.com/microsoft/dstoolkit-ai-ux/tree/main/visualizations/manufacturing_analytics",
  "toolkit_url" : "https://www.ds-toolkit.com/assets/cd9150dd-e8bf-402f-9bb7-08cfbfde225b",
  "scenarios": "Manufacturing plants have huge eletricity demands for which prices are negotiated in advance with electricity providers. Understanding the eletricity profiles of machines based on a set of factors, like speed, heating, etc, is the first step in optimizting plant operations.",
  "business_value": "- Cost reduction;Improved energy governance and control",
  "asset_description": "The live demo shows how to organise machinery data, what kind of dashboard stakeholders are interested to use, and how to predict energy consumption from machines.\"",
  "modeling_approach_and_training": "A simple trained linear regression model is added to the repository.",
  "dataset_summary": "Tabular data showing common machine controls like velocity, rotational speed, etc. The dataset is a dummy dataset based on past engagements.",
  "architecture": "Azure App Services, Streamlit",
  "topic": "Energy Profiling and Prediction",
  "explanation": "Energy profiling and prediction is a data science technique used to understand the electricity profiles of machines in manufacturing plants based on factors such as speed and heating. This helps optimize plant operations, reduce costs, and improve energy governance and control.",
  "short_explanation": "Energy profiling and prediction optimizes manufacturing plant operations, reduces costs, and improves energy governance and control.",
  "top_keywords": "energy consumption, machine data, cost reduction, energy governance, plant operations",
  "ip_id": 8
 },
 "9-9": {
  "ip_title": "Forecasting 2.0",
  "github_url": "https://github.com/microsoft/dstoolkit-forecasting",
  "toolkit_url" : "https://www.ds-toolkit.com/assets/bd53f8ba-0ff5-4a00-ab42-686ea5c67724",
  "scenarios": "Data preparation and model selection when trying to forecast several time series that show different behaviour, i.e. seasonality vs intermittence. Business cases: electricity demand forecast of several meters; eletricity production forecast of several plants; SKU demand in retail.",
  "business_value": "The forecasting accelerator helps to reduce the time taken for data preparation and model selection and to increase forecast accuracy",
  "asset_description": "This accelerator provides code and guidance to produce time series forecasting and time series profiling. This accelerator helps data scientists who want to forecast multiple time series by building models based on the time series profile (i.e. seasonal vs intermittent). Included in this accelerator are:\n1. Guidelines in the form of notebooks that can help with all the necessary steps to perform high quality data preparation, which is crucial in forecasting.\n2. A library of functions that help with:Sliding plots to visualize time series;Holidays by country or other regressors such as months, weekdays and interaction terms;Creating normal temperature future scenarios to generate forecasts years-ahead;Filling missing data using similar days or similar weeks values;Compute errors like mean absolute error and mean absolute percentage error (also in case of divede by zero);Wrap up results in Excel or csv files\n3. Help to undestand the complexity to forecast a time series by profiling the time series.\n4. Backtesting with multiple models, and choosing the best model in terms of mean absolute error\n5. Optimizing models to better fit the training dataset and forecast the target variable: from energy consumption to spare parts demand. Profiling time series helps in defining the best fitting model in terms of choice of regressors (calendar variables or temperatures), forecasting algorithm (ARIMA vs Exponential smoothing) and train set (one year or just few days of data)\"",
  "modeling_approach_and_training": "A Series of notebooks that walk through the essential steps to generate a good forecast: Exploration ; Preparation ; Profiling ; Clustering and ; Prediction scoring",
  "dataset_summary": "Energy Demand data is used to illustrate how to use the accelerator.",
  "architecture": "Azure Machine Learning, Jupyter Notebooks, Power BI (Energy Demand Dashboard), Python",
  "topic": "Time Series Forecasting",
  "explanation": "Time series forecasting is a technique used to predict future values based on historical data. This accelerator helps data scientists to forecast multiple time series by building models based on the time series profile, which is crucial in forecasting electricity demand, electricity production, and SKU demand in retail.",
  "short_explanation": "This accelerator helps data scientists to forecast multiple time series by building models based on the time series profile.",
  "top_keywords": "time series, forecasting, data preparation, model selection, electricity demand",
  "ip_id": 9
 },
 "10-10": {
  "ip_title": "Fuzzy Matching",
  "github_url": "https://github.com/microsoft/dstoolkit-fuzzymatching-v2",
  "toolkit_url" : "https://www.ds-toolkit.com/assets/d95f68f3-2ac8-4966-8aad-b0237ed5668e",
  "scenarios": "Fuzzy matching is used in situations in which data-driven insights are required from disparate data sources but in which there are missing keys, messy data and/or unstructured data.",
  "business_value": "Relating data that does not have a primary-key relationship is helpful in many use cases, for example matching product SKUs for master data, matching people to skills for project management etc",
  "asset_description": "Best practices and resources for implementing fuzzy matching solutions. The repository is broken down into the following sections.Data Management (record linkage);People Matching;Knowledge Graphs",
  "modeling_approach_and_training": "Jupyter notebooks use sample data of company names from Nasdaq and S&P 500 data and use fuzzywuzzy and dask to implement fuzzy matching at scale.",
  "dataset_summary": "Kaggle Sample dataset is provided.",
  "architecture": "Azure Machine Learning, Jupyter Notebooks, Python",
  "topic": "Fuzzy Matching",
  "explanation": "Fuzzy matching is a technique used to match data from different sources that do not have a primary-key relationship. It is useful in scenarios where there are missing keys, messy data, or unstructured data. For example, it can be used to match product SKUs for master data or match people to skills for project management.",
  "short_explanation": "Fuzzy matching matches data from different sources without a primary-key relationship. Useful for matching product SKUs or people to skills.",
  "top_keywords": "fuzzy matching, data integration, record linkage, data matching, knowledge graphs",
  "ip_id": 10
 },
 "11-11": {
  "ip_title": "Glue - Cognitive Services Accelerator",
  "github_url": "https://github.com/microsoft/glue",
  "toolkit_url" : "https://www.ds-toolkit.com/assets/10855150-f093-4952-9501-c0b6089e4138",
  "scenarios": "- Automated generation of synthetic speech-model training data.;Batch transcription of audio files and evaluation against a reference transcript.",
  "business_value": "Relating data that does not have a primary-key relationship is helpful in many use cases, for example matching product SKUs for master data, matching people to skills for project management etc",
  "asset_description": "Glue consists of a series of Python scripts and an orchestrator to enable batch processing by using Microsoft Cognitive Services. This accelerator enables: Audio files to be transcribed to text (for analysis of contact center conversations);Synthesis of text data to speech for text-to-speech applications.;Evaluation of reference transcripts and recognitions.;Scoring of text strings on an existing, pre-trained Microsoft LUIS model.",
  "modeling_approach_and_training": "Microsoft Cognitive Services",
  "dataset_summary": "Audio and text files",
  "architecture": "Microsoft Speech to Text Service (STT), Microsoft Text to Speech Service (TTS), Pre-trained Microsoft LUIS-model, Python, VSCode, ffmpeg",
  "topic": "Batch Processing with Microsoft Cognitive Services",
  "explanation": "Batch processing with Microsoft Cognitive Services allows for automated generation of synthetic speech-model training data, batch transcription of audio files, and evaluation against a reference transcript. This can be useful for analyzing contact center conversations, text-to-speech applications, and scoring text strings on a pre-trained Microsoft LUIS model.",
  "short_explanation": "Automated batch processing with Microsoft Cognitive Services for speech-model training data and audio transcription.",
  "top_keywords": "batch processing, Microsoft Cognitive Services, speech-model training data, audio transcription, LUIS model",
  "ip_id": 11
 },
 "12-12": {
  "ip_title": "Hierarchical multi-label classification",
  "github_url": "https://github.com/microsoft/dstoolkit-hierarchical-multilabel-classification",
  "toolkit_url" : "https://www.ds-toolkit.com/assets/91b5840c-e207-4624-bee3-d0c5513a278a",
  "scenarios": "Create a hierarchical multi-label classifier, which explores all the approaches possible in hierarchical multi-label classification and recommends the best method.",
  "business_value": "Supports variety of use-cases in which classes are hierarchically structured and objects can be assigned to multiple paths of the class hierarchy at the same time, ex. text classification & image annotation",
  "asset_description": "The labeling and layering in a hierarchical, multi-label classification (HMLC) model can be quite complex and take a significant amount of time, especially in discovering the best approach. This accelerator gives the best approach with all the models wrapped in a single model-like wrapper.",
  "modeling_approach_and_training": "There are broadly three approaches to solving a multi-label classification: power-set labels, independent models and chained models. This accelerator explores all these approaches with different algorithms or model classes passed on by the users and returns a composite model-like artefact, which wraps the best approach and constituent models with the familiar Sklearn-like methods.",
  "dataset_summary": "Labelled data in the form of Pandas dataframes containing input columns and label columns. Types of data that can be processed: Text;Numerical data (say, financial variance analysis);Image data after pre-processing",
  "architecture": "Azure Machine Learning, Python,",
  "topic": "Hierarchical Multi-Label Classification",
  "explanation": "Hierarchical Multi-Label Classification is a machine learning technique that allows for assigning multiple labels to a single instance while taking into account the hierarchical structure of the labels. This technique is useful in scenarios where classes are hierarchically structured and objects can be assigned to multiple paths of the class hierarchy at the same time, such as text classification and image annotation.",
  "short_explanation": "Hierarchical Multi-Label Classification assigns multiple labels to a single instance while considering the hierarchical structure of the labels. Useful for text classification and image annotation.",
  "top_keywords": "Hierarchical, Multi-Label, Classification, Machine Learning, Text Classification, Image Annotation",
  "ip_id": 12
 },
 "13-13": {
  "ip_title": "Knowledge Graph Enabled Search",
  "github_url": "https://github.com/microsoft/dstoolkit-kg-search",
  "toolkit_url" : "https://www.ds-toolkit.com/assets/298b661e-47b0-4799-8b1e-5c889768ab36",
  "scenarios": "Most organizations have large amount of information residing in multiple systems. When information is required by the organization's employees, they rely on querying different systems with different search mechanisms, and/or reaching out to different SMEs in the organization. This trial-and-error exercise relies on the search capabilities of each system and is time consuming.",
  "business_value": "Technology has advanced and much of our focus has been on consolidating the data into Data Lakes, whereas, bringing meaningful information is the key to democratizing knowledge, and ultimately improving the overall efficiency and consistency of business operations.",
  "asset_description": "This accelerator provides the code template to implement a domain-aware search solution that uses knowledge graph (KG) to enrich the organic results of a general-purpose search engine, with the goal of reducing the time to information and improving productivity of users by bringing forth the most relevant search result beyond the specified search keywords.",
  "modeling_approach_and_training": "Building domain-specific Named Entity Recognition (NER) model to detect entities of interest that are present in search queries. Leveraging Knowledge Graph for: Search refinement - Refine search results by recognizing different meanings of a search term. Search expansion - Expand search results through the relationships of the detected entities",
  "dataset_summary": "This accelerator supports structured and unstructured data that can be indexed by a general-purpose search engine, e.g., text, image, video, etc. The accelerator assumes that a knowledge graph is available, and it contains the entities and relationships that can be used to support targeted search refinement or/and search expansion scenarios",
  "architecture": "Azure Cognitive Search, Azure Cosmos DB, Azure Machine Learning",
  "topic": "Knowledge Graphs for Search",
  "explanation": "Knowledge graphs are a way to enrich search results by connecting related information and providing more context. This can improve the efficiency and consistency of business operations by reducing the time it takes to find relevant information.",
  "short_explanation": "Using knowledge graphs to improve search efficiency and consistency.",
  "top_keywords": "knowledge graph, search, efficiency, consistency, information",
  "ip_id": 13
 },
 "14-14": {
  "ip_title": "Knowledge Mining Accelerator",
  "github_url": "https://github.com/microsoft/dstoolkit-km-solution-accelerator",
  "toolkit_url" : "https://www.ds-toolkit.com/assets/3b4fafba-7e09-4557-b0b5-1985a27684d7",
  "scenarios": "Unstructured data volume is overwhelming business users as well as harming their productivity by generating unnecessary insights, searchability & discoverability obstacles.",
  "business_value": "End-to-End solution to mine unstructured data into business high-value actionable with better insights, full searchability & great discoverability.",
  "asset_description": "Knowledge mining (KM) is an emerging discipline in artificial intelligence (AI) that uses a combination of intelligent services to quickly learn from vast amounts of information. It allows organizations to deeply understand and easily explore information, uncover hidden insights, and find relationships and patterns at scale.",
  "modeling_approach_and_training": "Approach take for Knowledge mining with this accelerator:Ingestion from Azure Data Lake; Enrichment with Azure Applied AI and Cognitive Services; Exploration; Keyword and Semantic search; Content security model (permissions); Tables extraction including Key-Values; OCR all images-based content; Deep document search to page or slide level",
  "dataset_summary": "This accelerator provides some sample data to highlight our mining capabilities including non-English documents. The users can upload or copy their own data in the documents container to evaluate more data mining scenarios.",
  "architecture": "Azure Cognitive Search, Azure Cognitive Services,Azure Functions, Azure App Services, Azure Storage",
  "topic": "Knowledge Mining",
  "explanation": "Knowledge Mining is an AI-based discipline that helps organizations to quickly learn from vast amounts of unstructured data and uncover hidden insights, relationships, and patterns at scale. It provides an end-to-end solution to mine unstructured data into high-value actionable insights with better searchability and discoverability.",
  "short_explanation": "Knowledge Mining is an AI-based discipline that helps organizations to quickly learn from vast amounts of unstructured data and uncover hidden insights, relationships, and patterns at scale.",
  "top_keywords": "AI, unstructured data, hidden insights, relationships, patterns",
  "ip_id": 14
 },
 "15-15": {
  "ip_title": "Large Scale Route Optimization",
  "github_url": "https://github.com/microsoft/dstoolkit-route-optimization",
  "toolkit_url" : "https://www.ds-toolkit.com/assets/e98ab392-b244-4e50-a81f-a9d54c6a135b",
  "scenarios": "Creating an optimization engine that can automatically suggest the best route for product delivery.",
  "business_value": "Increased delivery pace and cost decrease.",
  "asset_description": "This accelerator provides the code template to solve large-scale route optimization problems.",
  "modeling_approach_and_training": "Route optimization is modelled as a constraint programming problem, that can take on arbitrary real-world constraints. A parallelization framework is implemented for finding feasible solutions of large-scale optimization problems.",
  "dataset_summary": "This accelerator accepts data in a structured format, e.g., in CSV format. The data is expected to provide the information that can be used to formulate the route optimization problem. It also accepts a few user defined parameters, e.g., maximum number of stops per vehicle, for simulation purposes.",
  "architecture": "Azure Machine Learning",
  "topic": "Large Scale Route Optimization",
  "explanation": "Large Scale Route Optimization is a data science topic that involves creating an optimization engine to suggest the best route for product delivery. This can lead to increased delivery pace and cost savings.",
  "short_explanation": "Large Scale Route Optimization for efficient product delivery",
  "top_keywords": "route optimization, product delivery, cost savings, optimization engine, large-scale",
  "ip_id": 15
 },
 "16-16": {
  "ip_title": "ML Ops",
  "github_url": "https://github.com/microsoft/dstoolkit-mlops-base",
  "toolkit_url" : "https://www.ds-toolkit.com/assets/858322f5-9b8f-48b5-8b4d-8ac5a23b2c76",
  "scenarios": "Many AI projects never reach the production stage and do not deliver expected business results due to a lack of common approach in a multi-disciplinary team .",
  "business_value": "Increased speed to production; KPI project timeline, sprint cadence; Improved system tracking and reporting; Cost reduction; KPI Azure infrastructure costs",
  "asset_description": "This accelerator provides a template for repeatable, consistent and easy to deploy MLOps to bring ML models to production and help data science teams to focus on AI aspects and specific customer nuances and enabling multi-disciplinary team collaboration.",
  "modeling_approach_and_training": "The workflow uses Azure Devops for the CI/CD pipelines and manages artefacts (model and data) replication in different environments. It also leverages the AzureML python library to manage the run automated model training, store the model artefacts and create inference endpoints",
  "dataset_summary": "This accelerator leverage Azure ML Datasets to handle versioning and replication among different environments.",
  "architecture": "Azure Machine Learning, Azure DevOps",
  "topic": "MLOps",
  "explanation": "MLOps is a set of practices that combines machine learning and DevOps to help manage the entire machine learning lifecycle, from building and training models to deployment and monitoring in production. This accelerator provides a template for repeatable, consistent and easy to deploy MLOps to bring ML models to production and help data science teams to focus on AI aspects and specific customer nuances and enabling multi-disciplinary team collaboration.",
  "short_explanation": "MLOps is a set of practices that combines machine learning and DevOps to help manage the entire machine learning lifecycle, from building and training models to deployment and monitoring in production.",
  "top_keywords": "MLOps, machine learning, DevOps, deployment, monitoring",
  "ip_id": 16
 },
 "17-17": {
  "ip_title": "ML Ops for Databricks",
  "github_url": "https://github.com/microsoft/dstoolkit-mlops-databricks",
  "toolkit_url" : "https://www.ds-toolkit.com/assets/858322f5-9b8f-48b5-8b4d-8ac5a23b2c76",
  "scenarios": "Managing large teams of data engineers and data scientist presents challenges for collaboration and consistency of dev / test / prod environments.",
  "business_value": "The ML Ops for Databricks accelerator helps teams to collaborate, develop and deploy ML data prodcuts through a robust CI/CD process.",
  "asset_description": "The accelerator is fully configurable and can create up to 4 environments (Dev / UAT / Pre-prod / Prod): Full CI/CD between environments; Infrastructure as code interaction with Databricks API and Classification; Azure Service Principle authentication; Resource deployment using Bicep; Examples given within development framework using Python SDK for Databricks; Databricks Feature Store, MLFlow Tracking, Model Registry and Model Experiments.; DBX by Data Labs for Continuous Deployment of jobs, workflows; Databricks connect; Docker Environment",
  "modeling_approach_and_training": "Model agnostic",
  "dataset_summary": "Data type agnostic",
  "architecture": "Azure Databricks, Application Insights, Log analytics, Azure Key Vault, Azure Storage Account",
  "topic": "ML Ops",
  "explanation": "ML Ops is a set of practices and tools that help data science teams to collaborate, develop, and deploy machine learning models through a robust CI/CD process. The ML Ops for Databricks accelerator provides a fully configurable environment for managing large teams of data engineers and data scientists, ensuring consistency of dev/test/prod environments.",
  "short_explanation": "ML Ops for Databricks helps teams collaborate, develop, and deploy ML data products through a robust CI/CD process.",
  "top_keywords": "ML Ops, Databricks, collaboration, CI/CD, machine learning",
  "ip_id": 17
 },
 "18-18": {
  "ip_title": "Microsoft Presidio",
  "github_url": "https://github.com/microsoft/presidio",
  "toolkit_url" : "https://www.ds-toolkit.com/assets/c11fdf0c-229a-4360-a71e-88d65d4950c7",
  "scenarios": "Presidio allows organization to build de-identification pipelines which facilitate access to sensitive data; scanning of software systems and databases for personal information; semi-automated data redaction.",
  "business_value": "Enable risk redaction in data handling by de-identifying data; De-identify data on prem and on cloud to unblock integrations and migrations; De-identify data at move or at rest.",
  "asset_description": "na",
  "modeling_approach_and_training": "A toolbox for customizable de-identification pipelines, comprised of: Pesidio Analyzer: Identify PII/PHI in text, images, semi-structured data.; Presidio Anonymizer: Transform identified PII/PHI into non-sensitive data, using redaction, masking, encryption and more.; Presidio Image Redactor: Identify and redact PII/PHI in images using OCR.; Deployment and operationalization samples on multiple Azure platforms (Azure Data Factory, Azure App service, Azure Databricks, Azure Kubernetes Service and more); Presidio research: Tools for creating and evaluating PII detection mechanisms and for creating synthetic PII data.",
  "dataset_summary": "Any dataset containing PII/PHI",
  "architecture": "Python, Azure Data Factory, Databricks/Synapse, Azure Kubernetes Service, Azure App Services, On-premises/multiple clouds deployment, Jupyter notebooks",
  "topic": "De-identification of sensitive data",
  "explanation": "De-identification is the process of removing or masking personally identifiable information from data. Microsoft Presidio allows organizations to build de-identification pipelines, scan software systems and databases for personal information, and semi-automate data redaction. This enables risk reduction in data handling, unblocks integrations and migrations, and allows for de-identification of data at move or at rest.",
  "short_explanation": "Microsoft Presidio enables de-identification of sensitive data to reduce risk and unblock integrations and migrations.",
  "top_keywords": "De-identification, Sensitive data, Risk reduction, Integrations, Migrations",
  "ip_id": 18
 },
 "19-19": {
  "ip_title": "Power Virtual Agents Audio Codes",
  "github_url": "https://github.com/microsoft/dstoolkit-pva-audiocodes-cc",
  "toolkit_url" : "https://www.ds-toolkit.com/assets/2d168d2e-4a3f-4380-b236-03266e39aede",
  "scenarios": "The goal of a modern contact center solution is to provide a customer experience that rivals that of an experienced human agent and do that at scale and at a fraction of the cost of a traditional contact center. Achieving that goal requires a solution that can transform conversation into a coordinated business process that weaves together a core set of solution elements to respond effectively. This solution accelerator provides the building blocks and blueprint for how to use Power Virtual Agent and AudioCodes VoiceAI Connect Cloud Edition to build a modern contact center assistant. A modern contact center solution involves the effective integration of the key systems and services shown below leveraging, conversational AI and telephony. In this accelerator, the conversational AI discipline is provided by Microsoft's Power Virtual Agent (PVA) and telephony is provided by AudioCodes VoiceAI Connect Cloud (VAIC-C).",
  "business_value": "Provide cost savings and faster time to market for contact center solutions with this accelerator.",
  "asset_description": "The PVA AudioCodes accelerator supports the following components: Contact Center Assistant,; Experiences,; Customer Services, and; Line of Business Integration.",
  "modeling_approach_and_training": "Verseagility is a modular toolkit that can be extended by further use-cases as needed. Following use-cases are already implemented and ready to be used: Binary, multi-class & multi-label classification. Named entity recognition. Question answering. Opinion mining",
  "dataset_summary": "Audio data. Audio samples are provided in the repository in the folder AudioLogos",
  "architecture": "Power Virtual Agents, Power Automate, Azure Functions, Blob Storage, AudioCodes Voice AI Connect Cloud",
  "topic": "Conversational AI for Contact Centers",
  "explanation": "Conversational AI is a technology that enables machines to understand and respond to human language. This solution accelerator combines Microsoft's Power Virtual Agent and AudioCodes VoiceAI Connect Cloud to build a modern contact center assistant that can provide a customer experience that rivals that of an experienced human agent at a fraction of the cost.",
  "short_explanation": "Build a modern contact center assistant with Conversational AI using Power Virtual Agent and AudioCodes VoiceAI Connect Cloud.",
  "top_keywords": "Conversational AI, Contact Center, Power Virtual Agent, AudioCodes VoiceAI Connect Cloud, Customer Experience",
  "ip_id": 19
 },
 "20-20": {
  "ip_title": "Retail Analytics",
  "github_url": "https://github.com/microsoft/dstoolkit-retail-analytics",
  "toolkit_url" : "https://www.ds-toolkit.com/assets/2d168d2e-4a3f-4380-b236-03266e39aede",
  "scenarios": "To increase sales, companies need to identify both high value customers and customers who are likely to churn, in order to target these customers with marketing intervention that could influence behaviour, and thereby either increase spend in high value customers or decrease the likelyhood of customers churning.",
  "business_value": "Indentify high value customers.; Indentify customers who are likely to churn.; Understand what the uplift in sales could be when customer value is increased or when churn is decreased.",
  "asset_description": "Retail analytics uses recency, fequency and value metrics to segment customers and help determine which customers are more likely to be higher value customers (Customer Lifetime Value) and which customers are likely to churn. This can help marketing teams target sets of customers with specific messaging to prevent churn and increase customer value.",
  "modeling_approach_and_training": "Series of Notebooks that explain and provide examples for; Customer Segementation; Customer Lifetime Value prediction; Churn prediction; Predicting Sales; Designing A/B testing experiments",
  "dataset_summary": "Tabluar data. Sample opensource datasets are included in the repository.",
  "architecture": "Azure Machine Learning",
  "topic": "Customer Segmentation using RFM Analysis",
  "explanation": "RFM analysis is a data-driven customer segmentation technique that uses recency, frequency, and monetary value metrics to identify high-value customers and customers who are likely to churn. This helps marketing teams to target specific sets of customers with personalized messaging to prevent churn and increase customer value.",
  "short_explanation": "RFM analysis for customer segmentation",
  "top_keywords": "RFM analysis, customer segmentation, churn prevention, personalized messaging, customer value",
  "ip_id": 20
 },
 "21-21": {
  "ip_title": "Speech to Text Transcription and Classification",
  "github_url": "https://github.com/microsoft/dstoolkit-speech-to-text-nlp-mining-accelerator",
  "toolkit_url" : "https://www.ds-toolkit.com/assets/13598990-9f18-44e0-8b68-cbfbe74e2533",
  "scenarios": "Solves business use-cases including: General audio message classification within a noisy environment; Monitoring communications for safety and regulatory purposes; Complex audio message and communication classification scenarios",
  "business_value": "Automatically monitoring radio communications for safety and compliance purpose allows to intervene and improve adherence to protocol thereby improving safety outcomes.",
  "asset_description": "Speech-to-text transcription and classification solution accelerator is an end-to-end speech transcription and message classification solution for any type of business problem requiring an advanced method for classifying audio messages into actionable meaning.",
  "modeling_approach_and_training": "The proposed end-to-end solution workstreams flow consists of three main processes: audio signal processing, speech-to-text transcription/boosting and advanced NLP sequential modelling with communication conformance ML classification. Additionally, Azure ML Services and Pipelines are used to orchestrate these key processes.",
  "dataset_summary": "This accelerator provides some sample data in the use case of positive communication compliance monitoring in mining including audio files, ground truth transcription and class labels, as well as the domain specific ontology files. The users can refer to the format of the sample data and prepare their own ontology and data in their specific use case.",
  "architecture": "Azure Machine Learning",
  "topic": "Speech-to-Text Transcription and Classification",
  "explanation": "Speech-to-Text Transcription and Classification is a data science topic that involves converting spoken words into written text and categorizing them based on their meaning. This technology can be used to monitor radio communications for safety and compliance purposes, as well as classify complex audio messages into actionable insights.",
  "short_explanation": "Speech-to-Text Transcription and Classification converts spoken words into text and categorizes them. It can monitor radio communications for safety and compliance purposes and classify complex audio messages.",
  "top_keywords": "Speech-to-Text, Transcription, Classification, Radio Communications, Safety Compliance",
  "ip_id": 21
 },
 "22-22": {
  "ip_title": "Vitastic",
  "github_url": "https://github.com/microsoft/dstoolkit-speech-to-text-nlp-mining-accelerator",
  "toolkit_url" : "https://www.ds-toolkit.com/assets/0d7dd985-4e3a-4119-a90a-f1321d66fda5",
  "scenarios": "Create a web application that serves classification, object detection, and object segmentation models hosted in Azure custom vision and ML.",
  "business_value": "Support customizable Microsoft native design components using open source FluentUI framework, allow users to quickly build a web app with no front-end development prerequisites required.",
  "asset_description": "This accelerator helps to quickly build a web application that serves object detection, classification, and segmentation workloads. By leveraging Microsoft's themable React library FluentUI powered by Flask backend, Vitastic allows the users to build their own UI to demonstrate pre-trained models running in Azure.",
  "modeling_approach_and_training": "Vitastic serves computer vision models that are hosted in either Azure ML or custom vision. It allow the users to configure the models to be demonstrated, select the visualization style in use, and additional detection reports to be presented.",
  "dataset_summary": "A callable REST endpoint of pre-trained model should be prepared. In addition, users can provide up to 8 testing images used as template, as well as further images to be tested by the model.",
  "architecture": "Azure Machine Learning, Azure Custom Vision, Microsoft Fluent-UI framework, Python Flask framework",
  "topic": "Web Application for Object Detection and Segmentation using Azure Custom Vision and ML",
  "explanation": "Vitastic is a web application accelerator that allows users to quickly build a web app with no front-end development prerequisites required. It serves object detection, classification, and segmentation workloads by leveraging Microsoft's themable React library FluentUI powered by Flask backend. Users can demonstrate pre-trained models running in Azure.",
  "short_explanation": "Vitastic is a web app accelerator for object detection and segmentation using Azure Custom Vision and ML. It allows users to build their own UI to demonstrate pre-trained models running in Azure.",
  "top_keywords": "Web Application, Object Detection, Segmentation, Azure Custom Vision, ML",
  "ip_id": 22
 }
}
